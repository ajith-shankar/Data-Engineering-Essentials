{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b402c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a3607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/27 00:23:15 WARN Utils: Your hostname, ajith-Lenovo-G50-80 resolves to a loopback address: 127.0.1.1; using 192.168.1.40 instead (on interface wlp3s0)\n",
      "24/02/27 00:23:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/02/27 00:23:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/27 00:23:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FreshWorks Data Engineer\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a85bbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.40:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FreshWorks Data Engineer</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd9e824d9f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4b9f2",
   "metadata": {},
   "source": [
    "### ð…ð¢ð§ð ð­ð¡ðž ðð«ð¢ðœðž ðšð­ ð¬ð­ðšð«ð­ ð¨ðŸ ð­ð¡ðž ð¦ð¨ð§ð­ð¡ ðšð§ð ð­ð¡ðž ðð¢ðŸðŸðžð«ðžð§ðœðž ð¢ð§ ð­ð¡ðž ð©ð«ð¢ðœðž ðŸð«ð¨ð¦ ð©ð«ðžð¯ð¢ð¨ð®ð¬ ð¦ð¨ð§ð­ð¡ ðŸð¨ð« ðžðšðœð¡ ð¬ð¤ð®_ð¢ð"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c55ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe():\n",
    "    data = [\n",
    "            (1, '2023-01-01', 10),\n",
    "            (1, '2023-01-27', 13),\n",
    "            (1, '2023-02-01', 14),\n",
    "            (1, '2023-02-15', 15),\n",
    "            (1, '2023-03-03', 18),\n",
    "            (1, '2023-03-27', 15),\n",
    "            (1, '2023-04-06', 20),\n",
    "            (2, '2024-01-01', 50),\n",
    "            (2, '2024-01-29', 100),\n",
    "            (2, '2024-02-01', 150)\n",
    "        ]\n",
    "    \n",
    "    schema = [\"sku_id\", \"price_date\", \"price\"]\n",
    "    \n",
    "    return spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1afc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbaaf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|sku_id|price_date|price|\n",
      "+------+----------+-----+\n",
      "|     1|2023-01-01|   10|\n",
      "|     1|2023-01-27|   13|\n",
      "|     1|2023-02-01|   14|\n",
      "|     1|2023-02-15|   15|\n",
      "|     1|2023-03-03|   18|\n",
      "|     1|2023-03-27|   15|\n",
      "|     1|2023-04-06|   20|\n",
      "|     2|2024-01-01|   50|\n",
      "|     2|2024-01-29|  100|\n",
      "|     2|2024-02-01|  150|\n",
      "+------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "082da8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(df):\n",
    "    from pyspark.sql import Window\n",
    "    from pyspark.sql.functions import row_number, desc, when, dayofmonth, date_format, add_months, col, last, lag\n",
    "    \n",
    "    # sort the 'sku_id' based on 'price_date' on Descending order\n",
    "    sort_df = df.withColumn('row_num', row_number().over(Window.partitionBy('sku_id').orderBy(desc('price_date'))))\n",
    "\n",
    "    # if day of the 'price_date' is > firstDay, then add next month to the 'price_date'. Else keep the same month and date as fristDay\n",
    "    date_df = sort_df.withColumn('price_month', when(dayofmonth('price_date').__gt__(1), date_format(add_months('price_date', 1), 'yyyy-MM-01')) \\\n",
    "                                                     .otherwise(date_format('price_date', 'yyyy-MM-01'))\n",
    "                                )\n",
    "    \n",
    "    # group by 'sku_id' and month start price\n",
    "    group_df = date_df.groupBy(col('sku_id'), col('price_month').alias('price_date')).agg(last('price').alias('price'))\n",
    "    \n",
    "    # compare the price with previous month price and calculate the difference\n",
    "    final_df = group_df.withColumn('price_diff', col('price')-lag('price').over(Window.partitionBy('sku_id').orderBy('price_date')))\\\n",
    "                        .orderBy('sku_id', 'price_date')\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b461f149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+----------+\n",
      "|sku_id|price_date|price|price_diff|\n",
      "+------+----------+-----+----------+\n",
      "|     1|2023-01-01|   10|      null|\n",
      "|     1|2023-02-01|   14|         4|\n",
      "|     1|2023-03-01|   15|         1|\n",
      "|     1|2023-04-01|   15|         0|\n",
      "|     1|2023-05-01|   20|         5|\n",
      "|     2|2024-01-01|   50|      null|\n",
      "|     2|2024-02-01|  150|       100|\n",
      "+------+----------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outDf = solution(df)\n",
    "outDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7c490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
