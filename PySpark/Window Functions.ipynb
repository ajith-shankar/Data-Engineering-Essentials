{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04ca825-4476-4bbc-beba-d356aaae127e",
   "metadata": {},
   "source": [
    "# <font color=Blue>Window Functions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb15cd-b85a-40c2-ad7f-c4d22750ad7c",
   "metadata": {},
   "source": [
    "* Window functions operate on a group of rows and return a single value for every input row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dc3c3-3fbe-4aaf-b289-e81c8970f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3517dfc-ed86-4128-bc05-f4ff8d24fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "+-------------+----------+------+\n",
    "|employee_name|department|salary|\n",
    "+-------------+----------+------+\n",
    "|        James|     Sales|  3000|\n",
    "|      Michael|     Sales|  4600|\n",
    "|       Robert|     Sales|  4100|\n",
    "|        Maria|   Finance|  3000|\n",
    "|        James|     Sales|  3000|\n",
    "|        Scott|   Finance|  3300|\n",
    "|          Jen|   Finance|  3900|\n",
    "|         Jeff| Marketing|  3000|\n",
    "|        Kumar| Marketing|  2000|\n",
    "|         Saif|     Sales|  4100|\n",
    "+-------------+----------+------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c77670-1fd5-437e-8fdd-f7b64e3ccff8",
   "metadata": {},
   "source": [
    "### 1) row_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d2289-5a01-4ddf-a6bf-cd500ea7e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "res_df = df.withColumn(\"row_num\", row_number().over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f8333-a4e8-46ca-b976-995e9b8fb0c3",
   "metadata": {},
   "source": [
    "### 2) rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14cd17-7b31-486f-8444-fa20af1d2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "res_df = df.withColumn(\"rnk\", rank().over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e889b5d-7a24-456b-af53-5a9162d2af7d",
   "metadata": {},
   "source": [
    "### 3) dense_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9be73-6e47-41dd-bcab-f0739e0c59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "res_df = df.withColumn(\"dns_rnk\", dense_rank().over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a02367-be87-4b15-bc69-99c6721f7c57",
   "metadata": {},
   "source": [
    "### 4) percent_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6537ce7-1722-49d9-a35d-2261ce1d3ff4",
   "metadata": {},
   "source": [
    "* Returns the percentile rank of rows within a window partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43dac8-ebbe-4fdc-9754-66d06e9e283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "res_df = df.withColumn(\"dns_rnk\", percent_rank().over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622f701-4b41-4540-9686-185d862d1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output:\n",
    "+-------------+----------+------+------------+\n",
    "|employee_name|department|salary|percent_rank|\n",
    "+-------------+----------+------+------------+\n",
    "|        James|     Sales|  3000|         0.0|\n",
    "|        James|     Sales|  3000|         0.0|\n",
    "|       Robert|     Sales|  4100|         0.5|\n",
    "|         Saif|     Sales|  4100|         0.5|\n",
    "|      Michael|     Sales|  4600|         1.0|\n",
    "|        Maria|   Finance|  3000|         0.0|\n",
    "|        Scott|   Finance|  3300|         0.5|\n",
    "|          Jen|   Finance|  3900|         1.0|\n",
    "|        Kumar| Marketing|  2000|         0.0|\n",
    "|         Jeff| Marketing|  3000|         1.0|\n",
    "+-------------+----------+------+------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d60f7-5e1a-4103-a80a-f711e56d6284",
   "metadata": {},
   "source": [
    "### 5) ntile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf78fa-fc84-4295-b6bf-35a70d26fd69",
   "metadata": {},
   "source": [
    "* Returns the relative rank of result rows within a window partition\n",
    "* Below we have used 2 as a argument hence it retuns ranking between 2 values (1 and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817f9ff-8a46-4bf7-85a4-6cd27e77b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "res_df = df.withColumn(\"ntile\", ntile(2).over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ba2cc-a829-479c-b8d4-12fd28b6d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output:\n",
    "+-------------+----------+------+-----+\n",
    "|employee_name|department|salary|ntile|\n",
    "+-------------+----------+------+-----+\n",
    "|        James|     Sales|  3000|    1|\n",
    "|        James|     Sales|  3000|    1|\n",
    "|       Robert|     Sales|  4100|    1|\n",
    "|         Saif|     Sales|  4100|    2|\n",
    "|      Michael|     Sales|  4600|    2|\n",
    "|        Maria|   Finance|  3000|    1|\n",
    "|        Scott|   Finance|  3300|    1|\n",
    "|          Jen|   Finance|  3900|    2|\n",
    "|        Kumar| Marketing|  2000|    1|\n",
    "|         Jeff| Marketing|  3000|    2|\n",
    "+-------------+----------+------+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf27aa0-24f4-4c74-a4ad-5e061b7d0629",
   "metadata": {},
   "source": [
    "### 6) cume_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d7620-9768-43e4-a990-e1ad65ccfcba",
   "metadata": {},
   "source": [
    "* This function computes the cumulative distribution of the value within a window partition\n",
    "* The result ranges from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d767b4-0b5d-4eaf-862b-c29313b83d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import cume_dist\n",
    "\n",
    "res_df = df.withColumn(\"cumeDist\", cume_dist().over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984a1dd-063a-4b9d-a5f3-fa8a3a28c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output:\n",
    "+-------------+----------+------+------------------+\n",
    "|employee_name|department|salary|         cume_dist|\n",
    "+-------------+----------+------+------------------+\n",
    "|        James|     Sales|  3000|               0.4|\n",
    "|        James|     Sales|  3000|               0.4|\n",
    "|       Robert|     Sales|  4100|               0.8|\n",
    "|         Saif|     Sales|  4100|               0.8|\n",
    "|      Michael|     Sales|  4600|               1.0|\n",
    "|        Maria|   Finance|  3000|0.3333333333333333|\n",
    "|        Scott|   Finance|  3300|0.6666666666666666|\n",
    "|          Jen|   Finance|  3900|               1.0|\n",
    "|        Kumar| Marketing|  2000|               0.5|\n",
    "|         Jeff| Marketing|  3000|               1.0|\n",
    "+-------------+----------+------+------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020442a-cc3e-4d94-b512-863fb34644c5",
   "metadata": {},
   "source": [
    "### 7) lag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b6262-73a9-4cbf-82bd-e29ab9726d46",
   "metadata": {},
   "source": [
    "* lag() function allows you to access a previous row's value within the partition based on a specified offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2ff25-a9ce-4077-b45e-5c902258d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "res_df = df.withColumn(\"lag\", lag(\"salary\").over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29037df-d27a-47fe-9122-961675dbaeb4",
   "metadata": {},
   "source": [
    "### 8) lead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d52bf50-be82-42f5-a3ce-a5e773336d0b",
   "metadata": {},
   "source": [
    "* lead() retrieves the column value from the following row within the partition based on specified offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422a3d5-00ca-4387-9ce9-ec89a6f323ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "res_df = df.withColumn(\"lag\", lead(\"salary\", 2).over(Window.partitionBy(\"department\").orderBy(\"salary\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
