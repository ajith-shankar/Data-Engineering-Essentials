{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4837f0e6",
   "metadata": {},
   "source": [
    "# <font color=Blue>Databricks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5df038",
   "metadata": {},
   "source": [
    "- Azure Databricks is a popular cloud-based data analytics service offered by Microsoft Azure\n",
    "- It allows you to perform data analytics on huge amounts of data on Azure\n",
    "- Azure Databricks cluster uses Spark Standalone cluster\n",
    "- Control pane holds metadata information like, databricks web app, notebooks, jobs & queris, cluster manager\n",
    "- Compute pane holds data, Vnet, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6b49",
   "metadata": {},
   "source": [
    "![databricks](./Databricks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cd556",
   "metadata": {},
   "source": [
    "## 1) Databricks Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48f771",
   "metadata": {},
   "source": [
    "* Cluster is a set of computation resources and configurations to run your workloads\n",
    "* There are 2 types of cluster\n",
    "     - All purpose Cluster\n",
    "     - Job Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098aed15",
   "metadata": {},
   "source": [
    "### 1.1) All Purpose Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a1985",
   "metadata": {},
   "source": [
    "* To interactively run the commands in your notebook\n",
    "* Multiple users can share such clusters to do collaborative interactive analysis\n",
    "* You can terminate, restart, attach, detach these clusters to multiple notebooks\n",
    "* You can choose:\n",
    "    * Multi-Node cluster: Driver and executor nodes will be on seperate machine\n",
    "    * Single Node Cluster: Only there will be a single Driver with single machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b82e3a3",
   "metadata": {},
   "source": [
    "### 1.2) Job Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4789c07",
   "metadata": {},
   "source": [
    "* To run the job that you can run as a automated workflows\n",
    "* It runs a new job cluster and terminate the cluster automatically when the job is complete\n",
    "* You cannot restart a job cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1ed6f",
   "metadata": {},
   "source": [
    "## 2) DBUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35c554",
   "metadata": {},
   "source": [
    "* Databricks provides set of utilities to efficiently interact with your notebook. The most commanly used DBUtils are\n",
    "    * File system Utilities\n",
    "    * Widget Utilities\n",
    "    * Notebook Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301fb6f-e89e-460b-a29d-a9f9594f1fe9",
   "metadata": {},
   "source": [
    "dbutils.widgets.text(name='text_name', defaultvalue='', label='Text Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37dd509-3e62-457d-8be8-d8831adead75",
   "metadata": {},
   "source": [
    "res = dbutils.widgets.get(text_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831e993-524f-4e58-9983-f704601e8bfb",
   "metadata": {},
   "source": [
    "# <font color=Blue>Delta Lake</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c40c5-af18-4f60-9467-0a7c8b619ed8",
   "metadata": {},
   "source": [
    "## Drawbacks of ADLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299e9d2-e2b6-4a36-8158-fcd80c3d3402",
   "metadata": {},
   "source": [
    "1. No ACID properties\n",
    "2. Job failures leads to inconsistent data\n",
    "3. Simultaneous write on same folder brings incorrect results\n",
    "4. No schema enforcement\n",
    "5. No support for updates (update & delete)\n",
    "6. No support for versioning\n",
    "7. Data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebd47d-7e69-40d9-8a13-fb2a94dc3612",
   "metadata": {},
   "source": [
    "## 1) What is Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243e4fe-b0ec-4955-ad3e-0588e11618a0",
   "metadata": {},
   "source": [
    "* Open source storage framework that brings reliability to data lakes\n",
    "* Brings **transactional** capabilities to data lakes\n",
    "* Runs on top of your existing data lake and support **parquet**\n",
    "* Enables **LakeHouse** architecture\n",
    "* Using **Delta Lake** we can implement LakeHouse architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7236a-7bd9-4ec3-b7e6-6eea954a04cd",
   "metadata": {},
   "source": [
    "abfss://container@storage_account.dfs.core.windows.net/folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496a96e-2056-4502-a296-0b4f90536f9b",
   "metadata": {},
   "source": [
    "## Read CSV from ADLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d43069-6c6b-4577-8945-37cc94bea684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "schema_1 = StructType([StructField(\"emp_name\", StringType()),\n",
    "                       StructField(\"emp_id\", IntegerType()),\n",
    "                       StrcutField(\"gender\", StringType())    \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7e765-e4c0-4bfc-bfb4-67de8c635414",
   "metadata": {},
   "source": [
    "df = (spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "                            .schema(schema_1)\n",
    "                            .load(\"abfss://container@adlsstorage.dfs.core.windows.net/folder/*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef9ad9-6232-4256-b6f7-0ed6d92128ce",
   "metadata": {},
   "source": [
    "## Write to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc27e1-a5be-4c5b-9a74-b3f4910c86fa",
   "metadata": {},
   "source": [
    "df.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"abfss://container@storage.dfs.core.windows.net/folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f52b9c-5357-4926-b4a0-f38b00515132",
   "metadata": {},
   "source": [
    "## Reading parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccccf01-d04c-498b-9b9c-ced94df904a2",
   "metadata": {},
   "source": [
    "df_1 = spark.read.format(\"parquet\") \\\n",
    "                .load(\"abfss://container@storage.dfs.core.windows.net/folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19608d3a-7672-4adb-bfbe-841c9969105c",
   "metadata": {},
   "source": [
    "## 2) Create Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def9c6e-763d-43ec-b285-57f1e5544cbb",
   "metadata": {},
   "source": [
    "df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"abfss://container@storage.dfs.core.windows.net/folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c5f1e-afe6-49fd-9516-847db0155233",
   "metadata": {},
   "source": [
    "* when we create delta format file, there will be two files\n",
    "* 1) _delta_log folder\n",
    "  2) snappy.parquet file\n",
    " <br>\n",
    "* _delta_log folder creates delta lake. It contains\n",
    "* 1) _tmp_path_dir folder\n",
    "  2) .crc checksum file\n",
    "  3) .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beb0d8-ae36-4c84-868a-1cac860fa342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
