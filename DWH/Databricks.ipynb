{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4837f0e6",
   "metadata": {},
   "source": [
    "# <font color=Blue>Databricks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5df038",
   "metadata": {},
   "source": [
    "- Azure Databricks is a popular cloud-based data analytics service offered by Microsoft Azure\n",
    "- It allows you to perform data analytics on huge amounts of data on Azure\n",
    "- Azure Databricks cluster uses Spark Standalone cluster\n",
    "- Control pane holds metadata information like, databricks web app, notebooks, jobs & queris, cluster manager\n",
    "- Compute pane holds data, Vnet, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff6b49",
   "metadata": {},
   "source": [
    "![databricks](./Databricks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cd556",
   "metadata": {},
   "source": [
    "## 1) Databricks Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48f771",
   "metadata": {},
   "source": [
    "* Cluster is a set of computation resources and configurations to run your workloads\n",
    "* There are 2 types of cluster\n",
    "     - All purpose Cluster\n",
    "     - Job Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098aed15",
   "metadata": {},
   "source": [
    "### 1.1) All Purpose Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a1985",
   "metadata": {},
   "source": [
    "* To interactively run the commands in your notebook\n",
    "* Multiple users can share such clusters to do collaborative interactive analysis\n",
    "* You can terminate, restart, attach, detach these clusters to multiple notebooks\n",
    "* You can choose:\n",
    "    * Multi-Node cluster: Driver and executor nodes will be on seperate machine\n",
    "    * Single Node Cluster: Only there will be a single Driver with single machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b82e3a3",
   "metadata": {},
   "source": [
    "### 1.2) Job Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4789c07",
   "metadata": {},
   "source": [
    "* To run the job that you can run as a automated workflows\n",
    "* It runs a new job cluster and terminate the cluster automatically when the job is complete\n",
    "* You cannot restart a job cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1ed6f",
   "metadata": {},
   "source": [
    "## 2) DBUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35c554",
   "metadata": {},
   "source": [
    "* Databricks provides set of utilities to efficiently interact with your notebook. The most commanly used DBUtils are\n",
    "    * File system Utilities\n",
    "    * Widget Utilities\n",
    "    * Notebook Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301fb6f-e89e-460b-a29d-a9f9594f1fe9",
   "metadata": {},
   "source": [
    "dbutils.widgets.text(name='text_name', defaultvalue='', label='Text Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37dd509-3e62-457d-8be8-d8831adead75",
   "metadata": {},
   "source": [
    "res = dbutils.widgets.get(text_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831e993-524f-4e58-9983-f704601e8bfb",
   "metadata": {},
   "source": [
    "# <font color=Blue>Delta Lake</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c40c5-af18-4f60-9467-0a7c8b619ed8",
   "metadata": {},
   "source": [
    "## Drawbacks of ADLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299e9d2-e2b6-4a36-8158-fcd80c3d3402",
   "metadata": {},
   "source": [
    "1. No ACID properties\n",
    "2. Job failures leads to inconsistent data\n",
    "3. Simultaneous write on same folder brings incorrect results\n",
    "4. No schema enforcement\n",
    "5. No support for updates (update & delete)\n",
    "6. No support for versioning\n",
    "7. Data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebd47d-7e69-40d9-8a13-fb2a94dc3612",
   "metadata": {},
   "source": [
    "## 1) What is Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243e4fe-b0ec-4955-ad3e-0588e11618a0",
   "metadata": {},
   "source": [
    "* Open source storage framework that brings reliability to data lakes\n",
    "* Brings **transactional** capabilities to data lakes\n",
    "* Runs on top of your existing data lake and support **parquet**\n",
    "* Enables **LakeHouse** architecture\n",
    "* Using **Delta Lake** we can implement LakeHouse architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7236a-7bd9-4ec3-b7e6-6eea954a04cd",
   "metadata": {},
   "source": [
    "abfss://container@storage_account.dfs.core.windows.net/folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496a96e-2056-4502-a296-0b4f90536f9b",
   "metadata": {},
   "source": [
    "### Read CSV from ADLS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d43069-6c6b-4577-8945-37cc94bea684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "schema_1 = StructType([StructField(\"emp_name\", StringType()),\n",
    "                       StructField(\"emp_id\", IntegerType()),\n",
    "                       StrcutField(\"gender\", StringType())    \n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7e765-e4c0-4bfc-bfb4-67de8c635414",
   "metadata": {},
   "source": [
    "df = (spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "                            .schema(schema_1)\n",
    "                            .load(\"abfss://container@adlsstorage.dfs.core.windows.net/folder/*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef9ad9-6232-4256-b6f7-0ed6d92128ce",
   "metadata": {},
   "source": [
    "### Write to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc27e1-a5be-4c5b-9a74-b3f4910c86fa",
   "metadata": {},
   "source": [
    "df.write.format(\"parquet\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"abfss://container@storage.dfs.core.windows.net/folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f52b9c-5357-4926-b4a0-f38b00515132",
   "metadata": {},
   "source": [
    "### Reading parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccccf01-d04c-498b-9b9c-ced94df904a2",
   "metadata": {},
   "source": [
    "df_1 = spark.read.format(\"parquet\") \\\n",
    "                .load(\"abfss://container@storage.dfs.core.windows.net/folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19608d3a-7672-4adb-bfbe-841c9969105c",
   "metadata": {},
   "source": [
    "## 2) Create Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def9c6e-763d-43ec-b285-57f1e5544cbb",
   "metadata": {},
   "source": [
    "df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(\"abfss://container@storage.dfs.core.windows.net/folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c5f1e-afe6-49fd-9516-847db0155233",
   "metadata": {},
   "source": [
    "* when we create delta format file, there will be two files\n",
    "* 1) _delta_log folder\n",
    "  2) snappy.parquet file\n",
    " <br>\n",
    "* **_delta_log** folder creates delta lake. It contains\n",
    "* 1.1) _tmp_path_dir folder\n",
    "* 1.2) .crc checksum file\n",
    "* 1.3) .json file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a9c8c7-e886-449c-9318-70aa9e79eebb",
   "metadata": {},
   "source": [
    "### Reading Delta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ec575-74cb-41ac-9e06-9581cae764ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\") \\\n",
    "                .load(\"abfss://container@storage.dfs.core.windows.net/folder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9d709-6d4c-4c3b-b161-93eb79549069",
   "metadata": {},
   "source": [
    "### Create Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59e9fd-a339-40b9-abb1-ee6b307719be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"`schema_name`.table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fdfccc-f50d-4b8d-b638-7bad9493ffd7",
   "metadata": {},
   "source": [
    "### Schema Enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf94cc-f42a-4c2e-b4a3-2739d8da3199",
   "metadata": {},
   "source": [
    "* Delta Lake uses Schema Validation on **Writes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9cda8-339e-4111-85a0-7430a996dc97",
   "metadata": {},
   "source": [
    "#### Schema enforcement Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a9aca-b7db-41fb-980a-404c583155cf",
   "metadata": {},
   "source": [
    "* Can't contain any additional columns that are not present in the target table's schema\n",
    "* Can't have different data type from the data type in the target table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03edc1-c0cb-49b3-89ad-2cae0f498597",
   "metadata": {},
   "source": [
    "### Schema Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb1508-e663-4304-9880-a90910a7ce2d",
   "metadata": {},
   "source": [
    "* It allows changes for additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59639c-150a-4c42-b503-7e01564495ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .saveAsTable(\"`schema_name`.table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583ce17-447c-4c3f-a532-73a7ea0d0d2b",
   "metadata": {},
   "source": [
    "* It allows changes for different schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae72f0-bb52-426a-84a6-c2ba2285a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writ.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(\"`schema_name`.table_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527a13e-74d8-4fc7-9284-8a9c8ac9b09c",
   "metadata": {},
   "source": [
    "### Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9241d66-2606-4ed2-b102-2e599a6b4e4f",
   "metadata": {},
   "source": [
    "### By using **versionAsOf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e0370-7206-4468-ad55-bb337fbe775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\") \\\n",
    "                .option(\"versionAsOf\", \"1\")\n",
    "                .load(\"dbfs:/user/hive/warehouse/db_name.db/table_name/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1eb75-0143-4e62-a4fc-d04ff6225bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\") \\\n",
    "                .option(\"timestampAsOf\", \"1\")\n",
    "                .load(\"dbfs:/user/hive/warehouse/db_name.db/table_name/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ba3fe-31f9-4d69-b13f-1b74089eb461",
   "metadata": {},
   "source": [
    "## Upsert in Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e9868-4be9-427d-a8e3-347622866bc6",
   "metadata": {},
   "source": [
    "### Upsert using Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282efb6d-c303-4185-9dd1-7389949a5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE INTO `bd_name`.Dest_Table_name as Dest\n",
    "USING Source_Table_Name as Source\n",
    "    ON Dest.key_col_name = Source.key_col_name\n",
    "  WHEN MATCHED \n",
    "    THEN UPDATE SET\n",
    "  Dest.col_1 = Source.Col_1,\n",
    "  Dest.col_2 = Source.col_2,\n",
    "    .           .\n",
    "    .           .\n",
    "  Dest.last_col = Source.last_col\n",
    "\n",
    "  WHEN NOT MATCHED\n",
    "    THEN INSERT\n",
    "       (col_1, col_2, col_3, col_4,...... last_col)\n",
    "        VALUES(Source.col_1, Source.col_2, Source.col_3,.......,Source.last_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd6e9d-69d1-4da7-87b8-0a0248b16bfc",
   "metadata": {},
   "source": [
    "# <font color=Blue>Unity Catalog</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d728bded-0fc6-4adc-804a-73852980a10d",
   "metadata": {},
   "source": [
    "* A centralized location where you can manage Users, Governance, Audit, Metadata management etc\n",
    "* Unity Catelog gives a unified governance layer into data & ai with a single permission model and it will give data sharing feature\n",
    "* Access Control, Lineage, Discovery, Monitoring, Auditing, Sharing, Metadata management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e69429-234b-45ac-8be2-a46664d22358",
   "metadata": {},
   "source": [
    "### Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea2cf0-f380-4b0a-9b9e-95a3b6eb3e79",
   "metadata": {},
   "source": [
    "* Metastore is a top level container in unity catalog. Within in Metastore a Unity catakog provides a 3 Level namespace for organizing a data\n",
    "* Catalog, Schema (database), Table\n",
    "* Only one metastore for one region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf916d-223a-4851-8c84-c4f0068a53a6",
   "metadata": {},
   "source": [
    "# <font color=Blue>Spark Structured Streaming</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909f3b3-d55f-4782-acb4-1365519c31a6",
   "metadata": {},
   "source": [
    "* Schema must be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9a050-3d50-46e9-93c6-f27a5551c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"csv\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .schema(schema) \\\n",
    "                    .load(\"abfss://folder/path/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0321d9-3bf9-4a67-bda6-ff5b2acac6c9",
   "metadata": {},
   "source": [
    "* most of the actions are not workinng for streaming\n",
    "* you can't read a file directly. Always read a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebdc16-8a7e-43b8-acfd-57976e1cf0a5",
   "metadata": {},
   "source": [
    "* File source\n",
    "* Kafka source\n",
    "* Table Source\n",
    "* Socket Source (UTF-8)\n",
    "* Rate Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c83cafc-39d1-48d0-b872-dfcfc843325f",
   "metadata": {},
   "source": [
    "* File Sink\n",
    "* Kafks Sink\n",
    "* Table Sink\n",
    "* Foreach Sink\n",
    "* Console Sink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5be29-a31e-4b7e-b86c-e7d8d04f34b4",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d8693-6480-4062-9c78-0e1287def92f",
   "metadata": {},
   "source": [
    "* To develop fault-tolerant and resilient saprk applications\n",
    "* It maintains intermediate state\n",
    "* It must be unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197784a-bb58-423f-820d-85acfdfcb5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write = df.writeStream.option(\"checkpointLocation\", \"checkpoit/path/\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .queryName(\"appendQuey\") \\\n",
    "            .toTable(\"schema_name.tablename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ce355-7d95-4c42-a1a2-d28f6cc49413",
   "metadata": {},
   "source": [
    "## outputMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac439d-086a-433f-b164-904fd2b749eb",
   "metadata": {},
   "source": [
    "* append\n",
    "* complete\n",
    "* update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44047074-7167-45a5-a20f-d571aaf6c30a",
   "metadata": {},
   "source": [
    "## trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedd5e48-f75a-4b70-9c9f-0f8504e4114b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m write \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoit/path/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      2\u001b[0m             \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      3\u001b[0m             \u001b[38;5;241m.\u001b[39mtrigger(processingTime \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2 minutes\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappendQuey\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;241m.\u001b[39mtoTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mschema_name.tablename\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "write = df.writeStream.option(\"checkpointLocation\", \"checkpoit/path/\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .trigger(processingTime = \"2 minutes\") \\\n",
    "            .queryName(\"appendQuey\") \\\n",
    "            .toTable(\"schema_name.tablename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7209d2-859d-40a9-a76c-fe30e088abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "write = df.writeStream.option(\"checkpointLocation\", \"checkpoit/path/\") \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .trigger(availableNow = True) \\\n",
    "            .queryName(\"appendQuey\") \\\n",
    "            .toTable(\"schema_name.tablename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c937f83-6431-4143-a899-221ba5d6b49b",
   "metadata": {},
   "source": [
    "# <font color=Blue>Auto Loader</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e880b7e-4b46-4d92-8513-52867f05265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"cloudFiles\") \\\n",
    "                    .option(\"coludFiles.format\", \"csv\") \\\n",
    "                    .option(\"clodFiles.schemaLocation\", \"path\") \\\n",
    "                    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .load(\"source_path\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
